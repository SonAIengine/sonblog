---
tags:
  - RAG
  - Kotaemon
  - ë¦¬íŒ©í† ë§
  - LLM
---

# OJT ë¦¬íŒ©í† ë§ê³¼ Kotaemon RAG êµ¬í˜„ê¸°

> 2025ë…„ 1ì›”, ê¸°ì¡´ OJT ì‹œìŠ¤í…œì˜ í•œê³„ë¥¼ í•´ê²°í•˜ê³ ì ì „ë©´ ë¦¬íŒ©í† ë§ì„ ì§„í–‰í–ˆë‹¤. íŠ¹íˆ Kotaemon RAG í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì„±ëŠ¥ì„ ëŒ€í­ ê°œì„ í•œ ê³¼ì •ì„ ê³µìœ í•œë‹¤.

## ê¸°ì¡´ ì‹œìŠ¤í…œì˜ í•œê³„

### OJT í”Œë«í¼ ë¬¸ì œì 
ê¸°ì¡´ OJT(On-the-Job Training) ì‹œìŠ¤í…œì€ ë‹¨ìˆœí•œ ë¬¸ì„œ ì €ì¥ì†Œì— ë¶ˆê³¼í–ˆë‹¤:

- **ê²€ìƒ‰ ì •í™•ë„ ë¶€ì¡±**: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ë§¥ë½ ì´í•´ ë¶ˆê°€
- **ë¬¸ì„œ íŒŒí¸í™”**: PDF, Word, PowerPoint ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì› ë¯¸í¡  
- **í•™ìŠµ ì¶”ì  í•œê³„**: ê°œë³„ í•™ìŠµìì˜ ì§„ë„ì™€ ì´í•´ë„ íŒŒì•… ì–´ë ¤ì›€
- **í™•ì¥ì„± ë¬¸ì œ**: ë¬¸ì„œ ì¶”ê°€ ì‹œë§ˆë‹¤ ìˆ˜ë™ ì¸ë±ì‹± í•„ìš”

íŠ¹íˆ ì‹ ì… ê°œë°œìë“¤ì´ "ì´ ê¸°ìˆ ì„ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€" ê°™ì€ ë§¥ë½ì  ì§ˆë¬¸ì— ëŒ€í•´ ì ì ˆí•œ ë‹µë³€ì„ ì–»ê¸° ì–´ë ¤ì› ë‹¤.

### RAG ì‹œìŠ¤í…œ í•œê³„
ê¸°ì¡´ì— êµ¬ì¶•í•œ RAGë„ ë¬¸ì œê°€ ìˆì—ˆë‹¤:

```python
# ê¸°ì¡´ RAG êµ¬ì¡°ì˜ ë¬¸ì œì 
class SimpleRAG:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vectordb = FAISS.from_documents(docs, self.embeddings)
        
    def query(self, question: str):
        # 1. ë‹¨ìˆœ ìœ ì‚¬ë„ ê²€ìƒ‰ë§Œ ì‚¬ìš©
        docs = self.vectordb.similarity_search(question, k=5)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì—†ì´ ëª¨ë“  ë¬¸ì„œ ì—°ê²°
        context = "\n".join([doc.page_content for doc in docs])
        
        # 3. ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸
        prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
        return llm.generate(prompt)
```

ë¬¸ì œì ë“¤:
- **ì²­í‚¹ ì „ëµ ë¶€ì¬**: ë¬¸ì„œë¥¼ ë¬´ì‘ì • 1000ìì”© ë‚˜ëˆ„ê¸°ë§Œ í•¨
- **ë©”íƒ€ë°ì´í„° ë¯¸í™œìš©**: ë¬¸ì„œ íƒ€ì…, ì‘ì„±ì¼, ë‚œì´ë„ ë“± ì •ë³´ ë¬´ì‹œ
- **ì¬ë­í‚¹ ì—†ìŒ**: ë‹¨ìˆœ ë²¡í„° ìœ ì‚¬ë„ë§Œìœ¼ë¡œ ë¬¸ì„œ ì„ íƒ
- **ë©€í‹°í„´ ì§€ì› ë¶€ì¡±**: ì´ì „ ëŒ€í™” ë§¥ë½ ê³ ë ¤ ì•ˆë¨

## Kotaemon RAG í”„ë ˆì„ì›Œí¬ ë„ì…

### Kotaemon ì„ íƒ ì´ìœ 

ì—¬ëŸ¬ RAG í”„ë ˆì„ì›Œí¬ë¥¼ ê²€í† í•œ ê²°ê³¼ Kotaemonì„ ì„ íƒí–ˆë‹¤:

| í”„ë ˆì„ì›Œí¬ | ì¥ì  | ë‹¨ì  |
|-----------|------|------|
| LangChain | ìƒíƒœê³„ í’ë¶€ | ë³µì¡ë„ ë†’ìŒ, ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ |
| LlamaIndex | ì¸ë±ì‹± ìµœì í™” | ì»¤ìŠ¤í„°ë§ˆì´ì§• ì–´ë ¤ì›€ |
| **Kotaemon** | **ëª¨ë“ˆëŸ¬ ì„¤ê³„, ì‰¬ìš´ í™•ì¥** | **ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ ì»¤ë®¤ë‹ˆí‹°** |
| Haystack | ê²€ìƒ‰ ì—”ì§„ í†µí•© | í•™ìŠµ ê³¡ì„  ê°€íŒŒë¦„ |

Kotaemonì˜ í•µì‹¬ ê°•ì :

1. **í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜**: ê° êµ¬ì„±ìš”ì†Œë¥¼ ë…ë¦½ì ìœ¼ë¡œ êµì²´ ê°€ëŠ¥
2. **ë©€í‹°ëª¨ë‹¬ ì§€ì›**: í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì²˜ë¦¬
3. **ì‹¤ì‹œê°„ í‰ê°€**: ë‹µë³€ í’ˆì§ˆì„ ì¦‰ì‹œ ì¸¡ì • ê°€ëŠ¥
4. **ì‰¬ìš´ ë°°í¬**: Docker ì»¨í…Œì´ë„ˆë¡œ ë°”ë¡œ ìš´ì˜ í™˜ê²½ ë°°í¬

### ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ì„¤ê³„

```python
# kotaemon ê¸°ë°˜ ìƒˆë¡œìš´ RAG ì•„í‚¤ï¿½ecture
from kotaemon.base import Document, BaseComponent
from kotaemon.retrievers import HybridRetriever
from kotaemon.generators import ReActAgent
from kotaemon.evaluation import RAGEvaluator

class AdvancedOJTRAG:
    def __init__(self):
        # 1. ë©€í‹° ë ˆë²¨ ì„ë² ë”© ì „ëµ
        self.dense_retriever = DenseRetriever(
            model_name="intfloat/multilingual-e5-large"
        )
        self.sparse_retriever = BM25Retriever()
        
        # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        self.hybrid_retriever = HybridRetriever(
            retrievers=[self.dense_retriever, self.sparse_retriever],
            weights=[0.7, 0.3]
        )
        
        # 3. ì¬ë­í‚¹ ëª¨ë¸
        self.reranker = CrossEncoderReranker(
            model_name="cross-encoder/ms-marco-MiniLM-L-12-v2"
        )
        
        # 4. ë©€í‹°í„´ ëŒ€í™” ê´€ë¦¬
        self.conversation_manager = ConversationManager()
        
        # 5. ì‹¤ì‹œê°„ í‰ê°€
        self.evaluator = RAGEvaluator(
            metrics=["faithfulness", "answer_relevancy", "context_precision"]
        )

    async def process_query(
        self, 
        query: str, 
        conversation_id: str = None,
        user_level: str = "beginner"
    ):
        # 1. ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        conversation_context = await self.conversation_manager.get_context(
            conversation_id
        )
        
        # 2. ì¿¼ë¦¬ í™•ì¥ ë° ì¬ì‘ì„±
        enhanced_query = await self._enhance_query(
            query, conversation_context, user_level
        )
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        candidate_docs = await self.hybrid_retriever.retrieve(
            enhanced_query, top_k=20
        )
        
        # 4. ì¬ë­í‚¹
        relevant_docs = await self.reranker.rerank(
            enhanced_query, candidate_docs, top_k=5
        )
        
        # 5. ë‹µë³€ ìƒì„±
        answer = await self._generate_answer(
            enhanced_query, relevant_docs, user_level
        )
        
        # 6. ì‹¤ì‹œê°„ í‰ê°€
        evaluation = await self.evaluator.evaluate(
            query, relevant_docs, answer
        )
        
        # 7. ëŒ€í™” ì €ì¥
        await self.conversation_manager.save_turn(
            conversation_id, query, answer, evaluation
        )
        
        return {
            "answer": answer,
            "sources": relevant_docs,
            "confidence": evaluation["faithfulness"],
            "suggestions": await self._get_suggestions(query, user_level)
        }
```

### ê³ ê¸‰ ì²­í‚¹ ì „ëµ

ê¸°ì¡´ì˜ ë‹¨ìˆœ ê³ ì • ê¸¸ì´ ì²­í‚¹ì„ ëŒ€ì²´í•˜ì—¬ ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹ì„ êµ¬í˜„í–ˆë‹¤:

```python
from kotaemon.parsers import SemanticChunker
from transformers import pipeline

class SmartDocumentProcessor:
    def __init__(self):
        self.semantic_chunker = SemanticChunker(
            chunk_size=512,
            chunk_overlap=50,
            separator_type="semantic"
        )
        
        # ë¬¸ì„œ íƒ€ì…ë³„ ì „ìš© íŒŒì„œ
        self.parsers = {
            'pdf': PDFParser(),
            'docx': DocxParser(), 
            'pptx': PowerPointParser(),
            'py': CodeParser(),
            'md': MarkdownParser()
        }
        
        # ë‚œì´ë„ ë¶„ì„ê¸°
        self.difficulty_classifier = pipeline(
            "text-classification",
            model="textbook-difficulty-classifier"
        )
    
    async def process_document(self, file_path: str, metadata: dict = {}):
        """ë¬¸ì„œë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹í•˜ê³  ë©”íƒ€ë°ì´í„° ì¶”ê°€"""
        
        # 1. íŒŒì¼ íƒ€ì…ë³„ íŒŒì‹±
        file_extension = file_path.split('.')[-1].lower()
        parser = self.parsers.get(file_extension, self.parsers['pdf'])
        
        content = await parser.parse(file_path)
        
        # 2. ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹
        chunks = await self.semantic_chunker.chunk(content)
        
        # 3. ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        enriched_chunks = []
        for chunk in chunks:
            # ë‚œì´ë„ ë¶„ì„
            difficulty = self.difficulty_classifier(chunk.content)[0]
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = await self._extract_keywords(chunk.content)
            
            # ì½”ë“œ ë¸”ë¡ ê°ì§€
            has_code = self._detect_code_blocks(chunk.content)
            
            enriched_chunk = Document(
                content=chunk.content,
                metadata={
                    **metadata,
                    'difficulty_level': difficulty['label'],
                    'difficulty_score': difficulty['score'],
                    'keywords': keywords,
                    'has_code': has_code,
                    'chunk_index': chunk.index,
                    'source_file': file_path,
                    'created_at': datetime.now().isoformat()
                }
            )
            enriched_chunks.append(enriched_chunk)
            
        return enriched_chunks
    
    async def _extract_keywords(self, text: str) -> list:
        """TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        import jieba  # í•œêµ­ì–´ í† í¬ë‚˜ì´ì§•
        
        # í˜•íƒœì†Œ ë¶„ì„
        tokens = jieba.cut(text)
        processed_text = ' '.join(tokens)
        
        # TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=10,
            stop_words='english',  # ì˜ì–´ ë¶ˆìš©ì–´
            ngram_range=(1, 2)
        )
        
        tfidf_matrix = vectorizer.fit_transform([processed_text])
        feature_names = vectorizer.get_feature_names_out()
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        scores = tfidf_matrix.toarray()[0]
        keywords = [
            feature_names[i] 
            for i in scores.argsort()[-5:][::-1]
            if scores[i] > 0
        ]
        
        return keywords
```

### ê°œì¸í™”ëœ í•™ìŠµ ê²½ë¡œ

ì‚¬ìš©ì ë ˆë²¨ì— ë”°ë¥¸ ë§ì¶¤í˜• ë‹µë³€ì„ ì œê³µí•˜ë„ë¡ êµ¬í˜„í–ˆë‹¤:

```python
class PersonalizedLearningAgent:
    def __init__(self):
        self.user_profiles = {}
        self.learning_graph = self._build_learning_graph()
    
    async def _generate_answer(
        self, 
        query: str, 
        docs: List[Document], 
        user_level: str
    ):
        """ì‚¬ìš©ì ë ˆë²¨ì— ë§ì¶˜ ë‹µë³€ ìƒì„±"""
        
        # ë ˆë²¨ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompts = {
            "beginner": """
ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì‹œë‹ˆì–´ ê°œë°œìì…ë‹ˆë‹¤. ì‹ ì… ê°œë°œìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ê·œì¹™:
1. ì „ë¬¸ ìš©ì–´ê°€ ë‚˜ì˜¤ë©´ ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…
2. êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì½”ë“œ ì œê³µ
3. ë‹¨ê³„ë³„ë¡œ ì°¨ê·¼ì°¨ê·¼ ì„¤ëª…
4. ê´€ë ¨ëœ ê¸°ì´ˆ ê°œë…ë„ í•¨ê»˜ ì–¸ê¸‰

ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {question}
ë‹µë³€:""",

            "intermediate": """
ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ê°œë°œìì…ë‹ˆë‹¤. ì‹¤ë¬´ ê²½í—˜ì´ ìˆëŠ” ê°œë°œìì—ê²Œ ì ì ˆí•œ ìˆ˜ì¤€ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ ì „ë‹¬
2. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ì™€ ì£¼ì˜ì‚¬í•­ í¬í•¨
3. ëŒ€ì•ˆì  ì ‘ê·¼ë²•ë„ ì œì‹œ
4. ì„±ëŠ¥ì´ë‚˜ ë³´ì•ˆ ê´€ì ì—ì„œì˜ ê³ ë ¤ì‚¬í•­ ì–¸ê¸‰

ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {question}
ë‹µë³€:""",

            "advanced": """
ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤. ê³ ê¸‰ ê°œë°œìì—ê²Œ ì‹¬ë„ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ê·œì¹™:
1. ì•„í‚¤í…ì²˜ ê´€ì ì—ì„œì˜ ë¶„ì„
2. íŠ¸ë ˆì´ë“œì˜¤í”„ì™€ ì„¤ê³„ ê²°ì • ê·¼ê±° ì œì‹œ
3. í™•ì¥ì„±, ìœ ì§€ë³´ìˆ˜ì„± ê³ ë ¤ì‚¬í•­
4. ìµœì‹  íŠ¸ë Œë“œë‚˜ ë°œì „ ë°©í–¥ ì–¸ê¸‰

ì»¨í…ìŠ¤íŠ¸: {context}
ì§ˆë¬¸: {question}
ë‹µë³€:""",
        }
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._format_context(docs, user_level)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = prompts[user_level].format(
            context=context,
            question=query
        )
        
        # LLM í˜¸ì¶œ
        response = await self.llm.agenerate(prompt)
        
        return response
    
    def _format_context(self, docs: List[Document], user_level: str) -> str:
        """ì‚¬ìš©ì ë ˆë²¨ì— ë§ì¶° ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        
        # ë‚œì´ë„ë³„ ë¬¸ì„œ í•„í„°ë§
        if user_level == "beginner":
            filtered_docs = [
                doc for doc in docs 
                if doc.metadata.get('difficulty_level') in ['beginner', 'intermediate']
            ]
        elif user_level == "intermediate":
            filtered_docs = docs  # ëª¨ë“  ë ˆë²¨
        else:  # advanced
            filtered_docs = [
                doc for doc in docs 
                if doc.metadata.get('difficulty_level') in ['intermediate', 'advanced']
            ]
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for doc in filtered_docs:
            source = doc.metadata.get('source_file', 'unknown')
            content = doc.content
            keywords = ', '.join(doc.metadata.get('keywords', []))
            
            context_parts.append(f"""
[ì¶œì²˜: {source}]
[í‚¤ì›Œë“œ: {keywords}]
{content}
""")
        
        return "\n\n".join(context_parts)
```

## ì‹œìŠ¤í…œ í†µí•© ë° ë°°í¬

### FastAPI ê¸°ë°˜ ì„œë¹„ìŠ¤

```python
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import Optional, List

app = FastAPI(title="OJT RAG API v2.0")

class QueryRequest(BaseModel):
    question: str
    conversation_id: Optional[str] = None
    user_level: str = "beginner"
    include_sources: bool = True

class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    confidence: float
    suggestions: List[str]
    conversation_id: str

@app.post("/query", response_model=QueryResponse)
async def process_query(request: QueryRequest):
    try:
        result = await rag_system.process_query(
            query=request.question,
            conversation_id=request.conversation_id,
            user_level=request.user_level
        )
        
        return QueryResponse(
            answer=result["answer"],
            sources=result["sources"] if request.include_sources else [],
            confidence=result["confidence"],
            suggestions=result["suggestions"],
            conversation_id=result.get("conversation_id", "")
        )
    except Exception as e:
        logger.error(f"Query processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload-document")
async def upload_document(
    file: UploadFile = File(...),
    metadata: dict = Body(...),
    background_tasks: BackgroundTasks
):
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¹„ë™ê¸° ì²˜ë¦¬"""
    
    # íŒŒì¼ ì €ì¥
    file_path = f"./uploads/{file.filename}"
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¬¸ì„œ ì²˜ë¦¬
    background_tasks.add_task(
        process_new_document,
        file_path,
        metadata
    )
    
    return {"message": "Document upload initiated", "file": file.filename}

async def process_new_document(file_path: str, metadata: dict):
    """ë°±ê·¸ë¼ìš´ë“œ ë¬¸ì„œ ì²˜ë¦¬"""
    try:
        # ë¬¸ì„œ íŒŒì‹± ë° ì²­í‚¹
        chunks = await doc_processor.process_document(file_path, metadata)
        
        # ë²¡í„° DBì— ì¶”ê°€
        await rag_system.add_documents(chunks)
        
        # ì²˜ë¦¬ ì™„ë£Œ ë¡œê·¸
        logger.info(f"Document processed successfully: {file_path}")
        
    except Exception as e:
        logger.error(f"Document processing failed: {file_path}, Error: {str(e)}")
```

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

```python
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
QUERY_COUNT = Counter('rag_queries_total', 'Total RAG queries', ['user_level'])
QUERY_LATENCY = Histogram('rag_query_duration_seconds', 'Query processing time')
RETRIEVAL_ACCURACY = Gauge('rag_retrieval_accuracy', 'Retrieval accuracy score')
ANSWER_CONFIDENCE = Histogram('rag_answer_confidence', 'Answer confidence scores')

class RAGMonitor:
    def __init__(self):
        self.metrics = {
            'query_count': QUERY_COUNT,
            'query_latency': QUERY_LATENCY,
            'retrieval_accuracy': RETRIEVAL_ACCURACY,
            'answer_confidence': ANSWER_CONFIDENCE
        }
    
    def record_query(self, user_level: str, latency: float, confidence: float):
        """ì¿¼ë¦¬ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.metrics['query_count'].labels(user_level=user_level).inc()
        self.metrics['query_latency'].observe(latency)
        self.metrics['answer_confidence'].observe(confidence)
    
    def update_accuracy(self, accuracy_score: float):
        """ê²€ìƒ‰ ì •í™•ë„ ì—…ë°ì´íŠ¸"""
        self.metrics['retrieval_accuracy'].set(accuracy_score)

# ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
@app.get("/metrics")
async def metrics():
    return Response(
        prometheus_client.generate_latest(),
        media_type="text/plain"
    )
```

## ì„±ëŠ¥ í‰ê°€ ë° ê²°ê³¼

### A/B í…ŒìŠ¤íŠ¸ ì„¤ì •

ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ìƒˆ ì‹œìŠ¤í…œì„ 3ì£¼ê°„ ë³‘ë ¬ ìš´ì˜í•˜ë©° ë¹„êµí–ˆë‹¤:

```python
# A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼
test_results = {
    "ê¸°ì¡´_RAG": {
        "ì •í™•ë„": 0.67,
        "ì‘ë‹µì‹œê°„": "2.3ì´ˆ",
        "ì‚¬ìš©ì_ë§Œì¡±ë„": 3.2,
        "ì¬ì§ˆë¬¸_ë¹„ìœ¨": 0.45
    },
    "Kotaemon_RAG": {
        "ì •í™•ë„": 0.89,
        "ì‘ë‹µì‹œê°„": "1.1ì´ˆ",
        "ì‚¬ìš©ì_ë§Œì¡±ë„": 4.6,
        "ì¬ì§ˆë¬¸_ë¹„ìœ¨": 0.18
    },
    "ê°œì„ ìœ¨": {
        "ì •í™•ë„": "+33%",
        "ì‘ë‹µì‹œê°„": "-52%",
        "ì‚¬ìš©ì_ë§Œì¡±ë„": "+44%",
        "ì¬ì§ˆë¬¸_ë¹„ìœ¨": "-60%"
    }
}
```

### ì£¼ìš” ê°œì„  ì‚¬í•­

1. **ê²€ìƒ‰ ì •í™•ë„**: 
   - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ë§¥ë½ ì´í•´ ëŠ¥ë ¥ í–¥ìƒ
   - ì¬ë­í‚¹ ëª¨ë¸ë¡œ ë…¸ì´ì¦ˆ ë¬¸ì„œ ì œê±°

2. **ë‹µë³€ í’ˆì§ˆ**:
   - ì‚¬ìš©ì ë ˆë²¨ë³„ ë§ì¶¤ ì„¤ëª…
   - ì½”ë“œ ì˜ˆì‹œì™€ ë‹¨ê³„ë³„ ê°€ì´ë“œ ì œê³µ

3. **í•™ìŠµ ì¶”ì **:
   - ê°œì¸ë³„ ëŒ€í™” ê¸°ë¡ ì €ì¥
   - í•™ìŠµ ì§„ë„ì™€ ì´í•´ë„ ë¶„ì„

## ìš´ì˜ ì¤‘ ë§ˆì£¼ì¹œ ì±Œë¦°ì§€

### 1. í† í° ê¸¸ì´ ì œí•œ
GPT-4ì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œìœ¼ë¡œ ê¸´ ë¬¸ì„œ ì²˜ë¦¬ê°€ ì–´ë ¤ì› ë‹¤:

```python
def smart_context_truncation(docs: List[Document], max_tokens: int = 16000):
    """ì¤‘ìš”ë„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë‹¨ì¶•"""
    
    # 1. ë¬¸ì„œë³„ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
    scored_docs = []
    for doc in docs:
        score = (
            doc.metadata.get('relevance_score', 0) * 0.4 +
            doc.metadata.get('recency_score', 0) * 0.3 +
            doc.metadata.get('authority_score', 0) * 0.3
        )
        scored_docs.append((score, doc))
    
    # 2. ì ìˆ˜ìˆœ ì •ë ¬ í›„ í† í° ì œí•œê¹Œì§€ ì„ íƒ
    sorted_docs = sorted(scored_docs, key=lambda x: x[0], reverse=True)
    
    selected_docs = []
    current_tokens = 0
    for score, doc in sorted_docs:
        doc_tokens = estimate_tokens(doc.content)
        if current_tokens + doc_tokens <= max_tokens:
            selected_docs.append(doc)
            current_tokens += doc_tokens
        else:
            break
    
    return selected_docs
```

### 2. ì‹¤ì‹œê°„ í‰ê°€ì˜ ì˜¤ë²„í—¤ë“œ
ëª¨ë“  ë‹µë³€ì— ëŒ€í•´ ì‹¤ì‹œê°„ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ë‹ˆ ì§€ì—°ì‹œê°„ì´ ì¦ê°€í–ˆë‹¤:

```python
class AdaptiveEvaluator:
    def __init__(self):
        self.evaluation_queue = asyncio.Queue()
        self.should_evaluate_realtime = self._should_evaluate_realtime
        
    async def conditional_evaluate(self, query: str, answer: str, docs: List[Document]):
        """ì¡°ê±´ë¶€ ì‹¤ì‹œê°„ í‰ê°€"""
        
        # ì‹¤ì‹œê°„ í‰ê°€ ì¡°ê±´
        if self.should_evaluate_realtime(query, answer):
            return await self.evaluator.evaluate(query, docs, answer)
        else:
            # ë°±ê·¸ë¼ìš´ë“œ í‰ê°€ íì— ì¶”ê°€
            await self.evaluation_queue.put({
                'query': query,
                'answer': answer,
                'docs': docs,
                'timestamp': time.time()
            })
            return {"confidence": 0.8}  # ê¸°ë³¸ê°’ ë°˜í™˜
    
    def _should_evaluate_realtime(self, query: str, answer: str) -> bool:
        """ì‹¤ì‹œê°„ í‰ê°€ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        
        # 1. ì¤‘ìš”í•œ ì§ˆë¬¸ (ë³´ì•ˆ, ì„¤ì • ë“±)
        important_keywords = ["ë³´ì•ˆ", "ì„¤ì •", "ë°°í¬", "ê¶Œí•œ", "ì—ëŸ¬"]
        if any(keyword in query for keyword in important_keywords):
            return True
            
        # 2. ì§§ì€ ë‹µë³€ (ë¹ ë¥¸ í‰ê°€ ê°€ëŠ¥)
        if len(answer) < 500:
            return True
            
        # 3. í™•ì‹ ë„ê°€ ë‚®ì€ ë‹µë³€
        if self._estimate_uncertainty(answer) > 0.3:
            return True
            
        return False
```

## ë§ˆë¬´ë¦¬

ì´ë²ˆ OJT ë¦¬íŒ©í† ë§ í”„ë¡œì íŠ¸ëŠ” ë‹¨ìˆœíˆ ê¸°ìˆ ì  ê°œì„ ì„ ë„˜ì–´ì„œ í•™ìŠµì ì¤‘ì‹¬ì˜ ì‚¬ê³ ë¥¼ í•˜ê²Œ ëœ ê³„ê¸°ì˜€ë‹¤. 

**í•µì‹¬ ê¹¨ë‹¬ìŒ:**
1. **ì‚¬ìš©ì ë ˆë²¨ ê³ ë ¤**: ê°™ì€ ì§ˆë¬¸ì´ë¼ë„ ê²½í—˜ì— ë”°ë¼ ë‹µë³€ ë°©ì‹ì´ ë‹¬ë¼ì•¼ í•¨
2. **ì ì§„ì  ê°œì„ **: ì™„ë²½í•œ ì‹œìŠ¤í…œì„ í•œ ë²ˆì— êµ¬ì¶•í•˜ê¸°ë³´ë‹¤ëŠ” ì¸¡ì •-ê°œì„ -ë°˜ë³µ
3. **ë„ë©”ì¸ íŠ¹í™”**: ë²”ìš© RAGë³´ë‹¤ëŠ” OJTì— íŠ¹í™”ëœ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ ë” íš¨ê³¼ì 

Kotaemon í”„ë ˆì„ì›Œí¬ì˜ ëª¨ë“ˆëŸ¬ ì„¤ê³„ ë•ë¶„ì— ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ì´í•‘í•  ìˆ˜ ìˆì—ˆê³ , ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•´ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•´ ë‚˜ê°ˆ ìˆ˜ ìˆì—ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„ë¡œëŠ” ë©€í‹°ëª¨ë‹¬ ì§€ì›(ì´ë¯¸ì§€, ì˜ìƒ ê¸°ë°˜ íŠœí† ë¦¬ì–¼)ê³¼ ì‹¤ì‹œê°„ í•™ìŠµ ê²½ë¡œ ì¶”ì²œ ê¸°ëŠ¥ì„ ê³„íší•˜ê³  ìˆë‹¤. ê°œë°œìì˜ í•™ìŠµ ì—¬ì •ì„ ë•ëŠ” AIì˜ ê°€ëŠ¥ì„±ì€ ë¬´ê¶ë¬´ì§„í•˜ë‹¤! ğŸ¯