
대형 언어 모델(LLM)의 활용이 확산됨에 따라, 효율적인 추론(inference) 및 서빙을 위한 인프라 선택이 점점 더 중요해지고 있다. 

본 글에서는 UC 버클리에서 개발된 두 가지 대표적인 추론 프레임워크인 **SGLang**과 **vLLM**을 다양한 관점에서 비교 분석한다. 

각각의 설계 철학, 핵심 기술, 성능, 멀티 GPU 지원 방식, 그리고 실제 활용 사례에 대해 다룬다.

## 1. 개요

### SGLang이란?

SGLang은 복잡한 LLM 기반 애플리케이션을 위한 추론 엔진으로, CPU 및 GPU 자원의 효율적인 활용을 통해 고성능을 지향한다. **다중 턴 대화**, **도구 호출**, **계획 기반 응답**, **JSON과 같은 구조화된 출력**을 자연스럽게 처리할 수 있도록 설계되었다.

- **정식 명칭**: Structured Generation Language
    
- **개발 기관**: UC Berkeley
    
- **주요 목적**:
    
    - 복잡한 LLM 프로그램(멀티턴 대화, 도구 호출 등)을 효율적으로 실행
        
    - 구조화된 출력(JSON 등) 지원
        
    - 프론트엔드 DSL과 백엔드 런타임의 공동 설계를 통해 멀티 GPU 확장성 확보
        

### vLLM이란?

vLLM은 고속 추론, 메모리 효율성, 손쉬운 통합을 목표로 하는 추론 프레임워크이다. 다양한 대형 언어 모델(Gemma, Qwen, GPT, DeepSeek 등)의 배치형 단일 요청 추론에 최적화되어 있다.

- **정식 명칭**: Vectorized Large Language Model Inference
    
- **개발 기관**: UC Berkeley
    
- **주요 목적**:
    
    - 단일턴 추론 시 메모리 효율성과 처리량 향상
        
    - PagedAttention 기반으로 고동시성과 고속 처리 구현
        
    - 기존 파이프라인과의 손쉬운 통합 지원


## 2. 핵심 기술 비교

|항목|SGLang|vLLM|
|---|---|---|
|핵심 기술|RadixAttention, DSL 기반 컴파일러 디자인|PagedAttention, Continuous Batching|
|구조화된 출력|지원(JSON, FSM 기반)|미지원|
|배치 처리|연속적 동적 배치 지원|Continuous Batching으로 자동화|
|캐시 최적화|Prefix 공유를 통한 캐시 재사용|PagedAttention 기반 GPU 메모리 최적화|

### SGLang 주요 기술

- **RadixAttention**: Radix Tree를 이용한 KV 캐시 관리 방식으로, 다중 턴 대화에서 prefix 공유를 통해 최대 5배까지 캐시 적중률을 향상시킨다.
    
- **구조화된 출력 지원**: 정규식 및 유한 상태 머신(FSM)을 활용한 제약 디코딩(constrained decoding)으로 JSON, XML 등 구조화된 출력이 가능하다.
    
- **컴파일러 영감 설계**: DSL을 이용해 복잡한 LLM 워크플로우를 선언적으로 기술할 수 있으며, 런타임에서 이를 최적화한다.

### vLLM 주요 기술

- **PagedAttention**: OS의 페이징 개념을 도입하여 KV 캐시를 고정 블록으로 분할, GPU 메모리의 동적 할당을 가능하게 한다.
    
- **연속 배치 처리(Continuous Batching)**: 요청을 prefill과 decode로 분리하여 실시간으로 배치 크기를 조정함으로써 GPU 활용률을 극대화한다.
    
- **Zero Redundancy Tensor Parallelism**: 중복 없는 가중치 분산 및 통신 최적화를 통해 멀티 GPU 환경에서 효율적인 추론이 가능하다.


## 3. 성능 및 사용 사례

### SGLang이 적합한 경우

- 멀티턴 대화형 시스템 (예: 챗봇, 에이전트 플래너)
    
- 출력 형식이 엄격히 정의된 경우 (예: JSON 기반 API 응답)
    
- Tool calling이나 복합적인 LLM 논리 흐름을 처리해야 하는 경우
    

**성능 지표**:

- LLaMA-7B 기준, vLLM 대비 5배 높은 멀티턴 처리량
    
- RadixAttention을 통해 30~50% 낮은 응답 지연(latency)

### vLLM이 적합한 경우

- 단일턴 Q&A, 콘텐츠 생성 등 대량의 요청을 빠르게 처리해야 하는 경우
    
- 리소스 제한 환경에서 최대의 처리량을 뽑아야 하는 경우
    
- HuggingFace Transformers 대비 14~24배 이상 높은 처리량
    
- GPU 한 대당 100개 이상의 동시 요청 처리 가능

## 4. 멀티 GPU 확장성

### SGLang의 멀티 GPU 전략

- **Tensor Parallelism**: `--tp 8` 과 같이 가중치를 다수 GPU에 분산
    
- **Data Parallelism**: 입력 데이터를 셰어링하여 GPU 간 부하 균형
    
- **RadixAttention 캐시 공유**: GPU 간 prefix 캐시를 공유하여 중복 연산 제거
    

### vLLM의 멀티 GPU 전략

- **Tensor Parallelism**: SGLang과 유사하나, 메모리 중복이 없음
    
- **Distributed Scheduler**: GPU 간 요청 라우팅 및 일부 요청 CPU 오프로딩
    
- **파이프라인 병렬화**: Kubernetes 클러스터 기반 멀티 노드 스케일링 지원

## 5. 선택 가이드

|상황|추천 프레임워크|
|---|---|
|복잡한 멀티턴 대화, 도구 호출|SGLang|
|JSON, XML 등 구조화된 출력이 필수인 경우|SGLang|
|높은 요청 동시성과 단일턴 처리|vLLM|
|빠른 구축과 쉬운 통합이 필요한 경우|vLLM|


## 6. 요약

|항목|SGLang|vLLM|
|---|---|---|
|핵심 강점|멀티턴, 구조화 출력, 복잡한 논리 처리|단일턴 고처리량, 메모리 효율|
|기술 스택|RadixAttention, DSL 기반 런타임 최적화|PagedAttention, 배치 최적화|
|사용 대상 모델|LLaMA, DeepSeek 등 범용 LLM/VLM|GPT-4, Mixtral 등 초대형 모델|
|학습 난이도|높음 (DSL 학습 필요)|낮음 (즉시 사용 가능)|

이 글은 프로젝트 성격, 처리 요구, 출력 포맷에 따라 SGLang과 vLLM 중 어떤 프레임워크가 적합한지를 빠르게 판단할 수 있도록 구성되었다. 각자의 기술적 강점을 바탕으로 두 시스템을 적절히 선택하는 것이 고성능 LLM 서빙 전략의 핵심이다.