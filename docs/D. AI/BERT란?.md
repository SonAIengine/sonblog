## 1. 셀프 어텐션의 전체 단계를 설명하라

### 셀프 어텐션, 왜 중요한가

셀프 어텐션은 입력 시퀀스 내부에서 각 토큰이 다른 모든 토큰과의 관계를 학습해 문맥 정보를 재구성하는 메커니즘이다. 이 과정을 거친 출력은 이후 피드포워드 네트워크나 다음 레이어로 전달되어 문장 수준의 의미를 정교하게 표현하게 된다.

### 0단계. 쿼리(Query)·키(Key)·밸류(Value) 추출 단계

- **무엇을 하는 단계인가**  
    입력 임베딩에 세 종류의 가중치 행렬을 곱해 쿼리, 키, 밸류 벡터를 생성하는 단계이다.
    
- **이 단계가 필요한 이유**  
    셀프 어텐션은 “질문·답변·정보”라는 세 가지 관점으로 입력을 바라본다. 쿼리는 질문자, 키는 질문에 응답할 수 있는 응답자, 밸류는 실제 정보를 담는 역할을 담당한다. 이렇게 역할을 분리함으로써 토큰 간 상호작용을 유연하게 모델링할 수 있다.
    
- **결과**  
    토큰마다 세 개의 벡터가 생기며, 이후 단계에서 상대적 중요도를 계산하는 재료가 된다.

### 1단계. 쿼리–키 내적 연산 단계

- **무엇을 하는 단계인가**  
    쿼리 벡터와 모든 키 벡터의 내적을 계산해 유사도 행렬을 얻는 단계이다.
    
- **내적을 사용하는 이유**  
    내적은 두 벡터의 방향 유사도(코사인 유사도에 비례)를 빠르게 측정할 수 있는 계산량이 적은 연산이다. 각 토큰이 자신과 다른 토큰 간 의미적 관련도를 직관적으로 파악할 수 있으므로 어텐션 가중치의 기초 자료로 적합하다.
    
- **결과**  
    행마다 n개의 스코어(각 토큰이 다른 토큰을 얼마나 참고할지에 대한 점수)를 포함한 행렬이 만들어진다.

### 2단계. 스케일링(√dk로 나누기) 단계

- **무엇을 하는 단계인가**  
    내적 결과를 키 벡터 차원의 제곱근 값으로 나누어 값을 축소하는 단계이다.
    
- **나누는 이유**  
    벡터 차원이 커질수록 내적값 분포가 넓어져 소프트맥스 입력이 과도하게 커질 수 있다. 값이 커지면 소프트맥스가 매우 작은 영역에서만 변화하여 학습이 불안정해진다. 차원의 제곱근으로 나누면 값의 범위를 적절히 좁혀 그래디언트 소실·폭발 위험을 줄일 수 있다.
    
- **결과**  
    수치적으로 안정적인 스코어 행렬을 다음 단계에 전달할 수 있다.


### 3단계. 소프트맥스 정규화 단계

- **무엇을 하는 단계인가**  
    스케일링된 스코어 행렬의 각 행에 소프트맥스를 적용해 확률 분포로 변환하는 단계이다.
    
- **소프트맥스를 사용하는 이유**  
    소프트맥스는 음수·양수 값을 모두 0~1 범위의 확률값으로 변환하고, 행마다 총합이 1인 분포를 만든다. 이를 통해 “어느 토큰에 얼마만큼 집중할지”를 직관적으로 해석할 수 있으며, 역전파 시 안정적 그래디언트를 보장한다.
    
- **결과**  
    각 토큰이 다른 토큰에 부여할 주목도가 명확한 확률 분포 형태로 완성된다.


### 4단계. 어텐션 가중합(Z) 계산 단계

- **무엇을 하는 단계인가**  
    소프트맥스로 얻은 가중치를 밸류 벡터들에 곱한 뒤 합산해 최종 어텐션 출력을 생성하는 단계이다.
    
- **이 단계가 필요한 이유**  
    가중합 결과는 각 토큰이 시퀀스 전반에서 얻은 문맥 정보를 반영한 새로운 표현이다. 이후 레이어에서 이 표현을 사용해 문장 의미를 더욱 깊이 있게 이해할 수 있다.
    
- **결과**  
    어텐션 출력 Z가 생성되며, 이는 잔차 연결·레이어 정규화를 거쳐 다음 서브레이어나 다음 트랜스포머 블록으로 전달된다.


0단계에서 쿼리·키·밸류를 분리 투영하고, 1단계와 2단계에서 유사도를 안정적으로 계산하며, 3단계에서 정규화된 가중치를 만들고, 4단계에서 문맥이 반영된 표현을 얻게 되는 흐름이 셀프 어텐션의 핵심이다. 이러한 단계적 설계 덕분에 트랜스포머 모델은 긴 문장에서도 관계를 효과적으로 학습하고, 번역·요약·질문 답변 등 다양한 자연어 처리 과제에서 높은 성능을 발휘하게 된다.

# Scaled Dot-Product Attention 를 정의하시오.

스케일드 닷 프로덕트 어텐션은 쿼리(Query), 키(Key), 밸류(Value) 삼중 구조를 사용해,
**유사도 계산 → 스케일 조정 → 확률 정규화 → 가중합**의 네 단계를 거쳐 문맥 정보를 추출하는 어텐션 메커니즘이다. 

트랜스포머 계열 모델에서 기본 블록으로 채택되는 이유는 계산 효율성과 병렬화 용이성에 있다.


### 1. 입력 구성 단계

- 입력 시퀀스의 각 토큰 임베딩을 **쿼리, 키, 밸류**로 독립 투영한다.
    
- 쿼리는 “질문자”, 키는 “후보 답변자”, 밸류는 “실제 답변 내용” 역할을 담당한다.
    

### 2. Dot-Product 유사도 계산 단계

- 한 토큰의 쿼리와 모든 토큰의 키 간에 내적(닷 프로덕트)을 수행해 유사도 행렬을 만든다.
    
- 내적은 두 벡터의 방향 유사도를 단일 숫자로 빠르게 환산하므로, 대규모 시퀀스에서 효율적이다.

### 3. 스케일링 단계

- 키 벡터 차원의 **제곱근으로 유사도 값을 나누어** 스케일을 조정한다.
    
- 차원이 클수록 내적값이 과도하게 커져 소프트맥스가 한두 항목에만 치우치는 현상이 생기므로, 이를 방지해 학습 안정성을 높인다.

### 4. 소프트맥스 정규화 단계

- 스케일링된 유사도 행렬에 **소프트맥스**를 적용해 각 행이 0~1 확률 분포가 되도록 만든다.
    
- 결과 분포는 “각 토큰이 다른 토큰에 얼마만큼 주목할지”를 나타내는 가중치 역할을 한다.

### 5. 가중합 단계

- 정규화된 가중치를 밸류 벡터에 곱해 **가중합**을 수행한다.
    
- 이 가중합 결과가 바로 각 토큰이 시퀀스 전체 문맥을 반영해 새로 얻은 표현(어텐션 결과)이다.


### 핵심 특징 정리

| 구분     | 목적              | 효과                    |
| ------ | --------------- | --------------------- |
| 닷 프로덕트 | 토큰 간 유사도 산출     | 계산량이 작고 GPU 병렬화에 적합하다 |
| 스케일링   | 큰 차원에서의 값 폭주 방지 | 그래디언트 안정성 확보          |
| 소프트맥스  | 가중치 확률 분포화      | 직관적 해석·역전파 용이         |
| 밸류 가중합 | 문맥 정보 결합        | 입력 토큰을 문맥 기반으로 재표현    |

스케일드 닷 프로덕트 어텐션은 이렇듯 “유사도 측정 → 스케일 조정 → 확률화 → 문맥 통합”의 명확한 절차를 통해, 입력 시퀀스 내부 관계를 효과적으로 모델링하는 핵심 구성 요소이다.



## Query, Key, Value 행렬은 어떻게 생성하는가?

### 1. 입력 임베딩 확보 단계
- 문장의 각 토큰을 임베딩 벡터로 변환한 뒤, 위치 인코딩을 더해 입력 행렬 **X**를 준비한다.  
- **X**의 크기는 `n × d_model`이며, *n*은 토큰 수, *d_model*은 모델 차원이다.

### 2. 전용 가중치 행렬 정의 단계
- 쿼리·키·밸류 각각에 대응하는 가중치 행렬 `W_Q`, `W_K`, `W_V`를 학습 가능한 파라미터로 초기화한다.  
- 일반적인 크기는 `d_model × d_h`이며, `d_h`는 헤드별 차원이다.

### 3. 선형 투영 수행 단계
- 입력 행렬 **X**에 각 가중치 행렬을 곱해 세 개의 행렬을 얻는다.  
  - `Q = X · W_Q`  
  - `K = X · W_K`  
  - `V = X · W_V`  
- 이 연산은 완전연결층과 동일하며 GPU 병렬화가 용이하다.

### 4. 멀티 헤드 분할 단계
- 표현 다양성을 위해 `h`개의 헤드로 분리한다.  
- 구현 방식  
  1. **병합 방식**: 한 번의 행렬 곱으로 `Q`, `K`, `V`를 `n × (h·d_k)` 크기로 만든 뒤 마지막 차원을 `(h, d_k)`로 리쉐이프한다.  
  2. **분리 방식**: 헤드마다 고유한 `(W_{Q_i}, W_{K_i}, W_{V_i})`를 두고 독립적으로 투영한다.

### 5. 학습과 업데이트 단계
- `W_Q`, `W_K`, `W_V`는 역전파를 통해 손실 함수의 그래디언트를 받아 지속적으로 갱신된다.  
- 옵티마이저(예: AdamW)가 가중치를 조정하여 토큰 간 의미 관계를 내재화한다.

> 위 절차를 통해 생성된 **Q**, **K**, **V** 행렬은 이후 스케일드 닷 프로덕트 어텐션에 입력되어 문맥 정보를 계산하게 된다.