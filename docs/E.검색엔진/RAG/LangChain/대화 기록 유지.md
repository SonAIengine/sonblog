## 1. 대화 이력이 중요한 이유

챗봇이 사용자의 질문을 기억하고 맥락에 맞게 답변하면 대화 경험이 자연스러워진다. 반대로 이전 메시지를 고려하지 못하면 질문-답변이 단절되어 사용자는 불편함을 느낀다. 따라서 **대화 이력을 효과적으로 관리하는 메모리 계층**은 실용적인 챗봇 개발에서 필수 요소이다.


## 2. 가장 단순한 방법: 프롬프트에 이력 그대로 넣기

가장 쉬운 접근은 이전 메시지들을 그대로 모델에 전달하는 방식이다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1) 프롬프트와 체인 구성
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "당신은 친절한 재무 상담가이다."),
        ("human", "{user_input}")
    ]
)
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.0)
chain = prompt | llm

# 2) 이전 대화 이력을 그대로 넣어 호출
ai_msg = chain.invoke(
    {
        "user_input": "저축을 늘리기 위해 무엇을 할 수 있나요?"
    }
)
print(ai_msg)
```

- **장점**: 구현이 쉽다.
    
- **단점**: 대화가 길어질수록 프롬프트 크기가 커져 토큰 비용이 증가하고 응답 속도가 느려진다.

## 3. 체계적인 이력 관리: `ChatMessageHistory` 활용

LangChain 등 라이브러리에서는 `ChatMessageHistory` 클래스를 제공한다. 이 객체는 대화 메시지를 리스트 형태로 저장하며, 새 질문이 올 때마다 `add_user_message`, `add_ai_message` 메서드로 이력을 갱신한다. 이렇게 하면 **저장, 조회, 삭제**가 명확해져 코드 유지보수가 수월하다.

```python
from langchain.memory import ChatMessageHistory
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# 1) 대화 이력 객체 생성
history = ChatMessageHistory()
history.add_user_message("저축을 늘리려면?")
history.add_ai_message("지출 분석부터 시작하라.")

# 2) 체인 정의
prompt = ChatPromptTemplate.from_messages(history.messages + [("human", "{question}")])
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.0)
chain = prompt | llm

# 3) 새 질문 처리
response = chain.invoke({"question": "신용카드 사용을 줄이는 방법은?"})
print(response)
```

---

## 4. 대화 이력 최적화 전략

### 4-1. 메시지 트리밍

가장 최근 N개 메시지만 남기고 오래된 기록을 삭제하는 방식이다.

```python
MAX_TURNS = 3          # 최근 3턴만 유지
PAIR = 2               # user+ai 한 쌍

if len(history.messages) > MAX_TURNS * PAIR:
    history.messages = history.messages[-MAX_TURNS * PAIR :]
```

- **장점**: 구현이 간단하며 최신 맥락은 유지된다.
    
- **단점**: 오래된 정보가 완전히 사라지므로 장기 의존성이 있는 대화에서는 맥락 손실이 발생할 수 있다.
    

### 4-2. 대화 요약

삭제 대신 **요약 메시지**로 압축해 보존하는 기법이다. 오래된 메시지들을 요약하고, 원문을 제거한 후 요약 결과만 남겨 두는 패턴이다.

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.output_parsers import StrOutputParser

# 1) 요약 체인 정의
summary_prompt = PromptTemplate(
    template=(
        "다음 대화를 두세 문장으로 요약하라. "
        "핵심 숫자와 고유명사는 그대로 유지하라.\n\n{dialogue}"
    ),
    input_variables=["dialogue"],
)
summarizer = summary_prompt | ChatOpenAI(model_name="gpt-4o-mini", temperature=0.0) | StrOutputParser()

# 2) 요약 대상 선정: 최근 3턴(6메시지)을 제외
old_msgs = history.messages[:-6]
dialogue_text = "\n".join([msg.content for msg in old_msgs])

# 3) 요약 실행
summary = summarizer.invoke({"dialogue": dialogue_text})

# 4) 원문 삭제 후 요약 메시지로 대체
history.messages = history.messages[-6:]       # 최근 3턴만 남김
history.add_system_message(f"요약: {summary}")
```

- **장점**: 맥락을 보존하면서 토큰 수를 크게 줄일 수 있다.
    
- **단점**: 요약 오류가 발생하면 핵심 정보가 왜곡될 수 있다. 따라서 모델 지시에 **“핵심 사실은 구체적인 숫자·고유명사로 남겨라”** 같은 제약을 명확히 주어야 한다.
    

---

## 5. 트리밍과 요약을 결합한 하이브리드 패턴

현실 서비스에서는 **“최근 대화는 원문 유지, 더 이전은 요약 보관, 그보다 오래된 것은 삭제”**와 같은 다단계 전략이 효과적이다.

|구간|처리 방안|
|---|---|
|최근 1 ~ 3턴|원문 유지|
|4 ~ 10턴 사이|요약 저장|
|10턴 이후|완전 삭제|

이렇게 하면 **즉시 필요한 맥락은 정확히**, **장기적인 이야기 흐름은 요약으로**, **불필요하게 긴 과거 데이터는 제거**할 수 있다.

---

## 6. 위험 요소와 모범 사례

|위험 요소|방지 방법|
|---|---|
|요약 과정의 정보 손실|중요 키워드·숫자를 그대로 남기도록 시스템 지시 추가|
|사용자 개인정보 노출|요약 전에 PII 마스킹 로직 삽입|
|토큰 한도 초과로 인한 오류|트리밍 또는 요약 후 토큰 길이 검증 함수로 최종 확인|
|멀티 세션 간 이력 충돌|사용자 ID별 분리된 메모리 저장소 사용|

---

## 7. 결론

대화형 AI의 품질은 **맥락 유지 능력**에서 결정된다.

- **프롬프트 직삽 방식**은 빠르지만 스케일에 한계가 있다.
    
- **`ChatMessageHistory`** 기반 구조화 저장은 유지보수를 돕는다.
    
- **트리밍**은 속도를, **요약**은 장기 맥락을 보존한다.
    
- 두 기법을 상황에 맞게 조합하면 **응답 지연·비용·맥락 손실**을 균형 있게 관리할 수 있다.
    

챗봇 개발자는 서비스 특성에 따라 **메시지 보존 주기, 요약 주체(모델·규칙 기반), 개인정보 처리 정책**을 명확히 정의해야 한다. 이를 통해 사용자는 끊김 없는 대화를 경험하고, 운영자는 리소스를 효율적으로 활용할 수 있다.