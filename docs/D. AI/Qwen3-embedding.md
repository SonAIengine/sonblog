## 인코더 기반이 아닌데도 잘하는 이유
### Decoder 아키텍처 활용 + [EOS] 벡터

Qwen3 기반 디코더 구조 위에, 문장 끝의 `[EOS]` 토큰 마지막 **히든 스테이트**를 바로 임베딩으로 사용.이렇게 하니 LLM의 풍부한 언어 이해 능력을 임베딩으로 그대로 흡수할 수 있습니다.


이게 무슨 말이냐?

### 먼저 토큰,히든 스테이트, 임베딩이 뭔데?

| 용어                        | 설명                            |
| ------------------------- | ----------------------------- |
| **토큰(token)**             | 문장을 잘게 자른 **단어 조각**           |
| **히든 스테이트(hidden state)** | 모델이 각 토큰을 보고 계산한 **머릿속 상태표**  |
| **임베딩(embedding)**        | 문장·단어를 고정 길이 숫자 벡터로 바꾼 **좌표** |
### 디코더는 어떻게 읽고 쓰나?
1. 토큰을 왼쪽 -> 오른쪽 순서로 한 개씩 넣으면서 다음 토큰을 예측
2. 
