텍스트 데이터들이 필요하다.

data 딕셔너리의 train 키에서 document 값을 가져와 모든 문서를 하나의 문자열로 합친다.
이렇게 만들어진 전체 텍스트에서 중복을 제거하고 정렬된 고유한 문자 목록을 생성한다.

이 과정을 통해 데이터셋에 존재하는 모든 고유한 한국어 문자를 파악할 수 있다.

그 다음, 이 고유 문자 목록의 길이를 계산해 전체 어휘 크기를 구하고, 총 글자 수를 출력해 데이터셋의 어휘 다양성을 확인합니다.

다음으로, 문자와 인덱스를 매핑하는 딕셔너리를 생성한다. 이러한 매핑은 텍스트 데이터를 숫자로 변환하고 다시 텍스트로 복원하는 데 사용된다.

 `데이터를 텐서로 변환하고 데이터 타입을 long으로 지정하는 과정은 딥러닝 모델의 효율적인 학습과 처리를 위해서 중요하다. 텐서는 딥러닝에서 데이터를 표현하는 기본 단위로, 다차원 배열 형태로 데이터를 저장하고 처리한다. 문자열을 숫자로 인코딩하고 이를 텐서로 변환하는 과정은 모델이 텍스트 데이터를 이해하고 학습할 수 있도록 하는 필수적인 전처리 단계이다.`
 
`데이터 타입을 long으로 지정하는 이유는 주로 텍스트 데이터의 특성과 관련이 있다. 텍스트 데이터를 숫자로 인코딩할 때 각 단어나 토큰에 해당하는 정수 값이 큰 범위를 가질 수 있기 때문이다. long 타입은 32비트 정수형보다 더 큰 범위의 정수를 표현할 수 있어 큰 어휘 사전을 다룰 때 유용하다. 또한, 파이토치의 많은 함수들이 기본적으로 long 타입의 인덱스를 기대하기 때문에 이를 사용하면 추후 처리 과정에서의 호환성을 보장할 수 있다.`

`이러한 텐서 변환과 데이터 타입 지정은 모델이 데이터를 효율적으로 처리하고 GPU를 통한 빠른 연산을 가능하게 함으로써 모델의 성능과 학습 속도에 직접적인 영향을 준다.`

데이터의 타입을 모두 변경한 후, 다음 단계로 train 데이터셋과 test 데이터셋으로 나누는 작업을 수행한다.
이 과정은 머신러닝 모델의 학습과 평가를 위해 매우 중요하다.

데이터를 훈련용과 검증용으로 분리함으로써 모델의 성능을 객관적으로 평가하고 과적합을 방지할 수 있다.
훈련 데이터는 모델을 학습하는 데 사용되며, 검증 데이터는 학습된 모델의 성능을 테스트하는 데 활용된다.

이러한 분리 작업을 통해 모델이 새로운, 보지 못한 데이터에 대해 얼마나 잘 일반화되는지 확인할 수 있다.
데이터 분할 비율은 일반적으로 프로젝트의 특성과 데이터의 양에 따라 결정되지만, 보통 8:2 또는 9:1등의 비율로 훈련 데이터과 검증 데이터를 나누기도 한다.

데이터를 분할 때 데이터를 처음부터 마지막까지 순차적으로 훈련한다고 생각할 수 있지만, 실제 학습 과정은 그렇지 않다. 훈련 데이터는 block_size에 설정된 크기만큼의 청크(chunk) 단위로 무작위 샘플링해 학습을 진행한다. 데이터 블록(block) 단위로 나누는 것은 GPT와 같은 트랜스포머 기반 모델을 학습할 때 자주 사용하는 방법이다.

여기서 `block_size` 는 한 번에 모델이 처리할 수 있는 글자의 수를 정의한다. 예를 들어 `block_size`  를 8로 설정하면 모델은 데이터의 연속된 8개의 글자를 하나의 학습 단위로 이용한다. 

이러한 방식으로 데이터를 처리함으로써 모델은 **다양한 문맥에서 언어를 이해하고 생성하는 능력**을 키울 수 있다. 

또한 무작위 샘플링을 통해 모델이 데이터의 특정 부분에 과적합되는 것을 방지하고, 전체 데이터셋에 대해 고르게 학습할 수 있도록 한다.

`block_size` 를 8로 설정하고 데이터가 하나씩 언어 모델의 입력으로 어떻게 전달되는지 살펴보겠다.
흔히 `block_size` 를 컨텍스트 길이(context length)라고 부른다. 즉, 모데링 한 번에 처리할 수 있는 토큰의 최대 길이를 의미하며, 이는 모델의 성능과 효율성에 큰 영향을 미친다. 이 값을 적절히 설정하는 것은 모델의 학습과 추론 과정에서 매우 중요하다. 큰 `block_size` 는 모델이 더 긴 문맥을 이해할 수 있게 해주지만, 동시에 더 많은 계산 자원을 필요로 한다. 반면 작은 `block_size` 는 계산 효율성은 높일 수 있지만, 모델의 문맥 이해 능력을 제한할 수 있다. 따라서 주어진 사용 가능한 자원을 고려해 적절한 `block_size` 를 선택하는 것이 중요하다.

**train_dataset[:block_size]** 이런 식으로 훈련 데이터셋의 처음 8개 글자를 텐서 형태로 보여줄 수 있다. 이 텐서는 숫자 배열이며 각 숫자는 특정 글자를 나타낸다. **학습 과정에 이런 블록은 트랜스포머 모델이 각 글자 뒤에 나타날 글자를 예측하도록 돕니다.** 모델은 각 위치에서 글자를 예측하며, 이 과정을 통해 문장 구조와 언어 패턴을 학습한다. 예를 들어 모델에 1928이라고 인코딩된 텍스트 정보를 입력했다고 가정해보자, 모델은 1928이라는 숫자로 인코딩된 텍스트를 봤다면 다음 글자 2315를 예측할 때 1928을 사용하고, 그 다음 글자인 0을 예측할 때는 1928과 2315를 함께 사용해 예측하도록 훈련한다.

```python
x = train_dataset[:block_size]
y = train_dataset[1:block_size+1]

for time in range(block_size):
	context = x[:time+1]
	target = y[time]

	print(f)
