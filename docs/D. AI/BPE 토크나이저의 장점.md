BPE(Byte Pair Encoding)는 텍스트를 처리하는 데 있어 효율성과 유연성을 동시에 제공하는 토큰화 방법이다. 특히 자연어 처리(NLP) 모델의 학습 및 추론 과정에서 다양한 장점을 가지므로, GPT 계열을 포함한 여러 언어 모델에서 널리 사용되고 있다.

### 1. 어휘 크기를 효과적으로 줄일 수 있다

BPE는 전체 단어 단위로 어휘를 구성하지 않고, 문자 또는 하위 문자열 단위에서 자주 등장하는 쌍을 반복적으로 병합함으로써 어휘 집합을 구성한다. 

이를 통해 전체 어휘 집합의 크기를 기존의 단어 기반 토크나이저보다 훨씬 작게 유지할 수 있으며, 모델 파라미터와 메모리 효율성을 높이는 데 기여한다.

### 2. 희귀 단어에 대한 대응력이 뛰어나다

기존의 단어 기반 토큰화 방식은 학습 데이터에 등장하지 않는 단어(OOV, Out-of-Vocabulary)에 대해 처리하지 못하는 문제가 있다. 

반면, BPE는 낯선 단어도 하위 문자열(subword) 단위로 분해하여 표현할 수 있으므로, OOV 문제를 효과적으로 완화할 수 있다.

이로 인해 모델의 일반화 성능이 향상되고, 사용자 입력에 대한 유연한 대응이 가능해진다.

### 3. 토큰 수를 줄여 연산 효율을 높일 수 있다

자주 등장하는 문자열을 하나의 토큰으로 병합하기 때문에, 동일한 문장을 기존의 문자 단위 혹은 단어 단위보다 적은 수의 토큰으로 표현할 수 있다. 

이는 학습 및 추론 시 처리해야 할 토큰 수를 줄이는 결과로 이어져, 연산 속도와 메모리 사용 측면에서 효율을 높일 수 있다.

### 4. 다국어 처리에 효과적이다

BPE는 언어에 관계없이 동일한 방식으로 적용 가능하므로, 다국어 텍스트를 처리하는 데 강점을 가진다. 특히 형태소 분석이 어려운 언어(예: 한글, 일본어, 중국어 등)에서도 일관된 방식으로 텍스트를 분해하고 학습할 수 있어, 다국어 언어 모델 개발에 적합하다.

### 5. 학습 안정성을 높일 수 있다

BPE는 긴 단어를 더 짧은 하위 단위로 나누기 때문에, 자주 등장하지 않는 단어들도 더 안정적으로 학습될 수 있다. 

이로 인해 모델은 드문 표현에 대해서도 일관된 표현력을 가질 수 있으며, 전반적인 학습 안정성과 추론 성능이 향상된다.


### 6. 예시: "국제시장과 국제적인"에 대한 BPE 처리 과정

입력 문장은 다음과 같다.

```
국제시장과 국제적인
```

#### 1단계: 초기 문자 단위 분할

BPE는 일반적으로 처음에는 한 글자(문자)를 하나의 토큰으로 본다.

```
국 제 시 장 과   국 제 적 인
```

이 상태에서 자주 등장하는 **문자 쌍**을 반복적으로 병합한다.


#### 2단계: 빈도 기반 문자 쌍 병합

입력 데이터에서 자주 등장하는 문자 쌍을 아래와 같이 병합해 간다.

1. `국` + `제` → `국제`
    
2. `시` + `장` → `시장`
    
3. `국제` + `시` + `장` → `국제시장` (데이터가 많아지면 이 단위도 병합 대상이 될 수 있음)
    
4. `국제` + `적` → `국제적`
    
5. `국제적` + `인` → `국제적인`

#### 최종 토큰화 결과 (예시)

| 원문    | 최종 토큰화 결과 (BPE) |
| ----- | --------------- |
| 국제시장과 | 국제, 시장, 과       |
| 국제적인  | 국제, 적, 인        |

※ 데이터가 충분하다면 `"국제적인"`을 하나의 토큰으로 병합하는 경우도 있을 수 있다.


### 요약

BPE는 다음과 같은 장점을 제공한다.

- "국제시장", "국제적인" 모두에서 `"국제"`가 공통적으로 등장하므로, 이를 하나의 토큰으로 처리하여 **중복 학습을 줄이고 의미를 일반화**할 수 있다.
    
- 희귀한 조합(예: "시장과", "적인")도 부분적으로 쪼개어 **OOV 없이 처리**할 수 있다.
    
- 동일한 접두어/접미어가 반복되는 한국어 특성상 **복합어 처리에 매우 효율적**이다.