
최근 자원-제약 환경에서 **정적 임베딩**(static embedding)이 재조명되고 있다. 트랜스포머 계열 모델이 성능 면에서 우위를 점하고 있음에도, 정적 임베딩은 **속도·메모리 이점**을 앞세워 적지 않은 품질 손실만으로도 충분히 실용적임을 보여 주고 있다. Qdrant 관점에서 그 활용 가능성과 실험 결과를 정리한다.

## 1. 정적 임베딩과 트랜스포머 임베딩의 차이

|구분|트랜스포머 기반 임베딩|정적 임베딩|
|---|---|---|
|**벡터 생성 방식**|입력 문장 전체를 인코딩해 **문맥 기반** 벡터 산출|각 토큰(또는 단어)을 **고정 벡터**로 매핑, 문장 벡터는 간단한 결합(평균 등)|
|**장점**|동음이의어를 맥락에 따라 구분최고 수준 검색 성능|모델 파라미터 수·메모리·연산량이 작음CPU 환경에서도 고속|
|**단점**|추론 시에도 GPU 의존이 높고 비용 부담|문맥 정보가 제한적 → 품질 열세|


## 2. 정적 임베딩의 부활 배경

### 2.1 model2vec: 2024년 10월 발표

- **MinishLab**의 _model2vec_ 기법
    
    - 트랜스포머 Sentence-T5를 distillation하여 **모델 크기 1/15, 추론 속도 최대 500배** 달성
        
    - MTEB 벤치마크에서 상위 80% 이상의 성능 유지
        

### 2.2 Hugging Face의 공개 파이프라인

- **Tom Aarsen** (HF 블로그, 2024-12)
    
    - Sentence Transformers로 **사용자 맞춤형 정적 모델**을 손쉽게 학습
        
    - 예시 모델 **`static-retrieval-mrl-en-v1`**:
        
        - 1,024차원, Matryoshka Representation Learning(MRL) 적용
            
        - SOTA 대비 **약 85% 성능**에 **수백 배 빠른** 인코딩

## 3. Qdrant에서의 활용

### 3.1 벡터 저장·검색

```python
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient, models
import uuid

model  = SentenceTransformer("sentence-transformers/static-retrieval-mrl-en-v1")
client = QdrantClient("http://localhost:6333")

client.create_collection(
    "my_collection",
    vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE),
)

client.upsert(
    "my_collection",
    points=[
        models.PointStruct(
            id=uuid.uuid4().hex,
            vector=model.encode("Hello, world!"),
            payload={"text": "Hello, world!"}
        )
    ],
)
```

- **검색 속도** 자체는 벡터 차원 수·인덱스 구조에 좌우되며 트랜스포머와 동일
    
- **인코딩 속도**는 CPU 환경 기준 최대 수백 배 향상 → 대규모 데이터셋 전처리 시간·비용 절감
    

### 3.2 양자화(Quantization) 실험

|BeIR 서브셋|원본 벡터|바이너리 양자화 후|
|---|---|---|
|SciFact|**0.5935**|0.5420|
|TREC-COVID|**0.4428**|0.4419|
|ArguAna|**0.4439**|0.4216|
|NFCorpus|**0.3004**|0.2803|

- **Binary Quantization** 적용 시 메모리·속도 이점 확보
    
- 일부 데이터셋은 품질 감소가 미미함 → 사용 환경에 따라 충분히 실용적

## 4. 도입을 고려할 만한 사례

|시나리오|기대 효과|
|---|---|
|**모바일·오프라인 앱**|작은 모델·낮은 전력 소모로 온-디바이스 임베딩 가능|
|**브라우저 확장(웹-GPU 불가 환경)**|JS/웹어셈블리로도 실시간 임베딩 수행|
|**임베디드·IoT**|한정된 CPU·RAM에서 유사 문서 검색 제공|
|**대규모 배치 전처리**|벡터 생성 병목 완화, 인코딩 비용 대폭 절감|


## 5. 커스터마이징과 학습

- **Sentence Transformers 정적 학습 파이프라인**
    
    - 대량의 도메인 코퍼스와 교차 엔코더 생성 라벨로 **핀튜닝 가능**
        
    - 학습 시간·자원 요구가 소형 트랜스포머보다 작음 → 주기적 재학습 용이
        
- **Matryoshka Embedding** 적용 시, 벡터 차원 축소(1024 → 256 등) 후에도 품질 유지

## 6. 결론

정적 임베딩은

- **환경 제약이 크거나 GPU 비용을 최소화**해야 하는 상황에서 특히 유용하다.
    
- 최신 기법(model2vec, MRL 등)이 적용되면 **품질 손실 ≤ 15%** 수준으로 **수-백 배 빠른 인코딩**을 실현한다.
    
- Qdrant는 별도 설정 없이 정적 벡터를 저장·검색할 수 있으며, 양자화·MRL을 통해 추가 최적화가 가능하다.
    

따라서 **“속도·경량화 > 최고 성능”** 인 프로젝트에서는 정적 임베딩을 적극 검토할 가치가 충분하다.