---
title: "Search APIì™€ LLMOps Docker êµ¬ì„±ê¸°"
description: "ê²€ìƒ‰ API ì„±ëŠ¥ ìµœì í™”ì™€ LLMOps íŒŒì´í”„ë¼ì¸ì˜ Docker í™˜ê²½ êµ¬ì„±ì„ ì§„í–‰í–ˆë‹¤. ì•„í‚¤í…ì²˜ ì„¤ê³„ì™€ ì»¨í…Œì´ë„ˆí™” ì „ëµ, docker-compose ë©€í‹° ì„œë¹„ìŠ¤ êµ¬ì„±ê¹Œì§€."
date: 2024-11-01
series: "DevOps ì‹¤ì „"
series_order: 1
difficulty: intermediate
tags:
  - Docker
  - LLMOps
  - ì¸í”„ë¼
  - CI/CD
  - docker-compose
  - ì»¨í…Œì´ë„ˆ
  - ê²€ìƒ‰API
  - ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
  - DevOps
  - ë°°í¬
related:
  - devops/infra/XGEN-2.0-ì¸í”„ë¼-K8s-ArgoCD-ìš´ì˜-ë°°í¬.md
  - devops/infra/Docker-BuildKit-ìºì‹œ-ì „ëµê³¼-NO_CACHE-ì˜µì…˜.md
  - devops/infra/K3s-ìœ„ì—-AI-í”Œë«í¼-ì˜¬ë¦¬ê¸°-ì¸í”„ë¼-ì„¤ê³„ë¶€í„°-ë°°í¬ê¹Œì§€.md
---
# Search APIì™€ LLMOps Docker êµ¬ì„±ê¸°

> 2024ë…„ 11ì›” í”„ë¡œì íŠ¸ì—ì„œ ê²€ìƒ‰ API ì„±ëŠ¥ ìµœì í™”ì™€ LLMOps íŒŒì´í”„ë¼ì¸ì˜ Docker í™˜ê²½ êµ¬ì„±ì„ ì§„í–‰í–ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë§ˆì£¼ì¹œ ì•„í‚¤í…ì²˜ ì„¤ê³„ì™€ ì»¨í…Œì´ë„ˆí™” ì „ëµì„ ê³µìœ í•œë‹¤.

## ë¬¸ì œ ìƒí™©

ê¸°ì¡´ XGEN 1.0 í”Œë«í¼ì—ì„œ ë²¡í„° ê²€ìƒ‰ APIê°€ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë¡œ ìš´ì˜ë˜ë©´ì„œ ì—¬ëŸ¬ ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤:

- **í™•ì¥ì„± ì´ìŠˆ**: ë™ì‹œ ìš”ì²­ ì¦ê°€ ì‹œ ì‘ë‹µ ì†ë„ ì €í•˜
- **ì˜ì¡´ì„± ì¶©ëŒ**: Python íŒ¨í‚¤ì§€ ë²„ì „ ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ í™˜ê²½ ë¬¸ì œ  
- **ë°°í¬ ë³µì¡ì„±**: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œ ì „ì²´ ì„œë¹„ìŠ¤ ì¬ì‹œì‘ í•„ìš”

íŠ¹íˆ RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ë²¡í„° ê²€ìƒ‰ê³¼ í…ìŠ¤íŠ¸ ìƒì„±ì´ í•˜ë‚˜ì˜ ì»¨í…Œì´ë„ˆì—ì„œ ì‹¤í–‰ë˜ì–´ ë¦¬ì†ŒìŠ¤ ê²½í•©ì´ ì‹¬ê°í–ˆë‹¤.

## ì•„í‚¤í…ì²˜ ì„¤ê³„

### ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬

ê¸°ì¡´ ëª¨ë†€ë¦¬ì‹ êµ¬ì¡°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ë¦¬í–ˆë‹¤:

```yaml
# docker-compose.yml
version: '3.8'
services:
  search-api:
    build: ./search-service
    ports:
      - "8001:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - VECTOR_DB_URL=qdrant:6333
    depends_on:
      - redis
      - qdrant
      
  llm-service:
    build: ./llm-service
    ports:
      - "8002:8000"
    environment:
      - MODEL_PATH=/models
      - GPU_MEMORY_FRACTION=0.7
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  qdrant:
    image: qdrant/qdrant:v1.6.1
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
```

### Search API ìµœì í™”

ê²€ìƒ‰ ì„œë¹„ìŠ¤ë¥¼ ë³„ë„ë¡œ ë¶„ë¦¬í•˜ê³  ì„±ëŠ¥ì„ ìµœì í™”í–ˆë‹¤:

```python
# search_service/app.py
from fastapi import FastAPI, HTTPException
from qdrant_client import QdrantClient
import asyncio
import numpy as np
from typing import List, Dict

app = FastAPI(title="Search API v2")

class SearchEngine:
    def __init__(self):
        self.qdrant = QdrantClient(url="http://qdrant:6333")
        self.redis = redis.Redis.from_url("redis://redis:6379")
        
    async def hybrid_search(
        self, 
        query: str, 
        collection: str,
        limit: int = 10,
        score_threshold: float = 0.7
    ) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©"""
        
        # 1. ë²¡í„° ê²€ìƒ‰
        vector_results = await self._vector_search(query, collection, limit*2)
        
        # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ (BM25)
        keyword_results = await self._keyword_search(query, collection, limit*2)
        
        # 3. RRF (Reciprocal Rank Fusion) ì ìš©
        combined_results = self._apply_rrf(vector_results, keyword_results)
        
        # 4. ìŠ¤ì½”ì–´ ì„ê³„ê°’ ì ìš©
        filtered_results = [
            r for r in combined_results 
            if r['score'] >= score_threshold
        ]
        
        return filtered_results[:limit]
    
    def _apply_rrf(self, vector_results, keyword_results, k=60):
        """RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ ìœµí•©"""
        doc_scores = {}
        
        # ë²¡í„° ê²€ìƒ‰ ì ìˆ˜
        for rank, result in enumerate(vector_results):
            doc_id = result['id']
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1/(k + rank + 1)
            
        # í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜  
        for rank, result in enumerate(keyword_results):
            doc_id = result['id']
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1/(k + rank + 1)
            
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        return [{'id': doc_id, 'score': score} for doc_id, score in sorted_docs]

@app.post("/search/hybrid")
async def hybrid_search(request: SearchRequest):
    try:
        results = await search_engine.hybrid_search(
            query=request.query,
            collection=request.collection,
            limit=request.limit,
            score_threshold=request.score_threshold
        )
        return {"results": results, "total": len(results)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### LLMOps íŒŒì´í”„ë¼ì¸

ëª¨ë¸ í•™ìŠµë¶€í„° ë°°í¬ê¹Œì§€ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ Dockerë¡œ í‘œì¤€í™”í–ˆë‹¤:

```dockerfile
# llm-service/Dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu22.04

# Python í™˜ê²½ ì„¤ì •
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    && rm -rf /var/lib/apt/lists/*

# CUDA í™˜ê²½ ë³€ìˆ˜
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=$CUDA_HOME/bin:$PATH
ENV LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ëª¨ë¸ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸
COPY scripts/optimize_model.py /app/scripts/
COPY src/ /app/src/

WORKDIR /app

# ëª¨ë¸ ë¡œë”© ìµœì í™”
ENV TRANSFORMERS_CACHE=/models/cache
ENV HF_HOME=/models/cache
ENV TORCH_CACHE_DIR=/models/cache

CMD ["python", "src/main.py"]
```

ëª¨ë¸ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸:

```python
# scripts/optimize_model.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from optimum.onnxruntime import ORTModelForCausalLM
import logging

def optimize_model(model_path: str, output_path: str):
    """ëª¨ë¸ ìµœì í™”: ONNX ë³€í™˜ + ì–‘ìí™”"""
    
    logger = logging.getLogger(__name__)
    
    # 1. ì›ë³¸ ëª¨ë¸ ë¡œë”©
    logger.info(f"Loading model from {model_path}")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # 2. ONNX ë³€í™˜
    logger.info("Converting to ONNX...")
    ort_model = ORTModelForCausalLM.from_pretrained(
        model_path,
        export=True,
        device_map="cuda:0"
    )
    
    # 3. ë™ì  ì–‘ìí™” ì ìš©
    from onnxruntime.quantization import quantize_dynamic, QuantType
    
    logger.info("Applying dynamic quantization...")
    quantized_model_path = f"{output_path}/model_quantized.onnx"
    quantize_dynamic(
        f"{output_path}/model.onnx",
        quantized_model_path,
        weight_type=QuantType.QInt8
    )
    
    # 4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    benchmark_results = benchmark_model(ort_model, tokenizer)
    logger.info(f"Optimization complete. Performance: {benchmark_results}")
    
    return {
        "model_path": quantized_model_path,
        "tokenizer_path": output_path,
        "performance": benchmark_results
    }

def benchmark_model(model, tokenizer, num_samples=100):
    """ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    import time
    
    prompt = "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # ì›Œë°ì—…
    for _ in range(10):
        with torch.no_grad():
            model.generate(**inputs, max_length=100, do_sample=False)
    
    # ì‹¤ì œ ì¸¡ì •
    start_time = time.time()
    for _ in range(num_samples):
        with torch.no_grad():
            outputs = model.generate(**inputs, max_length=100, do_sample=False)
    
    total_time = time.time() - start_time
    avg_latency = total_time / num_samples
    throughput = num_samples / total_time
    
    return {
        "avg_latency_ms": avg_latency * 1000,
        "throughput_samples_per_sec": throughput,
        "total_samples": num_samples
    }
```

## CI/CD íŒŒì´í”„ë¼ì¸

Jenkinsë¥¼ í™œìš©í•œ ìë™í™”ëœ ë°°í¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í–ˆë‹¤:

```groovy
// Jenkinsfile
pipeline {
    agent any
    
    environment {
        DOCKER_REGISTRY = 'registry.x2bee.com'
        KUBECONFIG = credentials('k3s-config')
    }
    
    stages {
        stage('Test') {
            steps {
                script {
                    sh 'docker-compose -f docker-compose.test.yml up --abort-on-container-exit'
                }
            }
        }
        
        stage('Build') {
            parallel {
                stage('Search API') {
                    steps {
                        script {
                            def searchImage = docker.build(
                                "${DOCKER_REGISTRY}/xgen/search-api:${BUILD_NUMBER}",
                                "./search-service"
                            )
                            searchImage.push()
                        }
                    }
                }
                stage('LLM Service') {
                    steps {
                        script {
                            def llmImage = docker.build(
                                "${DOCKER_REGISTRY}/xgen/llm-service:${BUILD_NUMBER}",
                                "./llm-service"
                            )
                            llmImage.push()
                        }
                    }
                }
            }
        }
        
        stage('Deploy') {
            steps {
                script {
                    sh '''
                        helm upgrade --install xgen-stack ./helm-chart \
                            --set searchApi.image.tag=${BUILD_NUMBER} \
                            --set llmService.image.tag=${BUILD_NUMBER} \
                            --namespace xgen \
                            --wait
                    '''
                }
            }
        }
    }
    
    post {
        success {
            slackSend(
                channel: '#xgen-deploy',
                message: "âœ… XGEN Stack v${BUILD_NUMBER} ë°°í¬ ì™„ë£Œ"
            )
        }
        failure {
            slackSend(
                channel: '#xgen-deploy',
                message: "âŒ XGEN Stack v${BUILD_NUMBER} ë°°í¬ ì‹¤íŒ¨"
            )
        }
    }
}
```

## ì„±ëŠ¥ ê²°ê³¼

ìµœì í™” í›„ ë‹¤ìŒê³¼ ê°™ì€ ì„±ëŠ¥ ê°œì„ ì„ í™•ì¸í–ˆë‹¤:

### ê²€ìƒ‰ API ì„±ëŠ¥
- **ì‘ë‹µ ì‹œê°„**: í‰ê·  1.2ì´ˆ â†’ 0.3ì´ˆ (75% ê°œì„ )
- **ë™ì‹œ ì²˜ë¦¬**: 50 RPS â†’ 200 RPS (4ë°° í–¥ìƒ)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 8GB â†’ 3GB (62% ê°ì†Œ)

### LLM ì„œë¹„ìŠ¤ ì„±ëŠ¥  
- **ì¶”ë¡  ì†ë„**: 15 tokens/sec â†’ 45 tokens/sec (3ë°° í–¥ìƒ)
- **GPU ë©”ëª¨ë¦¬**: 12GB â†’ 8GB (33% ì ˆì•½)
- **Cold start**: 30ì´ˆ â†’ 5ì´ˆ (83% ë‹¨ì¶•)

### í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì •í™•ë„
```python
# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼
test_results = {
    "precision_at_5": 0.87,      # ê¸°ì¡´: 0.72
    "recall_at_10": 0.94,        # ê¸°ì¡´: 0.81  
    "f1_score": 0.90,            # ê¸°ì¡´: 0.76
    "avg_query_time": "0.3s"     # ê¸°ì¡´: 1.2s
}
```

## ìš´ì˜ ê²½í—˜ê³¼ êµí›ˆ

### 1. ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
GPU ë©”ëª¨ë¦¬ í• ë‹¹ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì •í•´ì•¼ í•œë‹¤. ì´ˆê¸°ì— ì „ì²´ GPU ë©”ëª¨ë¦¬ë¥¼ í• ë‹¹í–ˆë‹¤ê°€ OOM ì—ëŸ¬ê°€ ë¹ˆë²ˆíˆ ë°œìƒí–ˆë‹¤.

```yaml
# GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: 1
          capabilities: [gpu]
    limits:
      memory: 16G
```

### 2. ëª¨ë¸ ìºì‹œ ì „ëµ
ëª¨ë¸ ë¡œë”© ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ ë‹¤ì¸µ ìºì‹œ êµ¬ì¡°ë¥¼ ì ìš©í–ˆë‹¤:
- **L1**: Redis ì¸ë©”ëª¨ë¦¬ ìºì‹œ (ìì£¼ ì‚¬ìš©í•˜ëŠ” ê²°ê³¼)
- **L2**: ë¡œì»¬ SSD ìºì‹œ (ëª¨ë¸ ê°€ì¤‘ì¹˜)
- **L3**: ë„¤íŠ¸ì›Œí¬ ìŠ¤í† ë¦¬ì§€ (ì „ì²´ ëª¨ë¸)

### 3. ëª¨ë‹ˆí„°ë§ í•„ìˆ˜ ë©”íŠ¸ë¦­
```python
# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
from prometheus_client import Counter, Histogram, Gauge

REQUEST_COUNT = Counter('search_requests_total', 'Total search requests')
REQUEST_LATENCY = Histogram('search_request_duration_seconds', 'Search request latency')
GPU_MEMORY_USAGE = Gauge('gpu_memory_usage_bytes', 'GPU memory usage')
MODEL_LOAD_TIME = Histogram('model_load_duration_seconds', 'Model loading time')
```

## ë§ˆë¬´ë¦¬

ì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ LLMOps íŒŒì´í”„ë¼ì¸ì˜ ì¤‘ìš”ì„±ì„ ë‹¤ì‹œ í•œë²ˆ ê¹¨ë‹¬ì•˜ë‹¤. ë‹¨ìˆœíˆ ëª¨ë¸ì„ ì„œë¹™í•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œ, ì „ì²´ ì¸í”„ë¼ë¥¼ ì½”ë“œë¡œ ê´€ë¦¬í•˜ê³  ìë™í™”í•˜ëŠ” ê²ƒì´ ì¥ê¸°ì ìœ¼ë¡œ í›¨ì”¬ íš¨ìœ¨ì ì´ë‹¤.

íŠ¹íˆ Docker ì»¨í…Œì´ë„ˆí™”ë¥¼ í†µí•´ ê°œë°œ-ìŠ¤í…Œì´ì§•-í”„ë¡œë•ì…˜ í™˜ê²½ì˜ ì¼ê´€ì„±ì„ ë³´ì¥í•  ìˆ˜ ìˆì—ˆê³ , ì´ëŠ” ë²„ê·¸ ì¶”ì ê³¼ ë””ë²„ê¹…ì— í° ë„ì›€ì´ ë˜ì—ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„ë¡œëŠ” Kubernetes ê¸°ë°˜ ì˜¤í† ìŠ¤ì¼€ì¼ë§ê³¼ A/B í…ŒìŠ¤íŠ¸ ìë™í™”ë¥¼ ê³„íší•˜ê³  ìˆë‹¤. MLOpsì˜ ì—¬ì •ì€ ê³„ì†ëœë‹¤! ğŸš€