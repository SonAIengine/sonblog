## 인코더 기반이 아닌데도 잘하는 이유
### Decoder 아키텍처 활용 + [EOS] 벡터

Qwen3 기반 디코더 구조 위에, 문장 끝의 `[EOS]` 토큰 마지막 **히든 스테이트**를 바로 임베딩으로 사용.이렇게 하니 LLM의 풍부한 언어 이해 능력을 임베딩으로 그대로 흡수할 수 있습니다.


이게 무슨 말이냐?

### 1. 먼저 토큰,히든 스테이트, 임베딩이 뭔데?

| 용어                        | 설명                            |
| ------------------------- | ----------------------------- |
| **토큰(token)**             | 문장을 잘게 자른 **단어 조각**           |
| **히든 스테이트(hidden state)** | 모델이 각 토큰을 보고 계산한 **머릿속 상태표**  |
| **임베딩(embedding)**        | 문장·단어를 고정 길이 숫자 벡터로 바꾼 **좌표** |
### 2. 디코더는 어떻게 읽고 쓰나?
1. 토큰을 왼쪽 -> 오른쪽 순서로 한 개씩 넣으면서 다음 토큰을 예측
2. 매 스텝마다 **"지금까지 본 모든 단어를 요약한 벡터"**(히든 스테이트)가 생김

즉, 마지막 스텝(문장 끝 `[EOS]`) 에서 계산된 히든 스테이트엔 
- **앞의 모든 단어 정보**
- **문장 전체 맥락**
이 압축되어 들어가 있다.

### 3. Qwen3-Embedding 은 이걸 이렇게 활용

> "다른 복잡한 후처리 말고, 마지막[EOS] 히든 스테이트 -> 바로 임베딩으로 쓰면 되지 않나?"

전체 문맥을 이미 담고 있으니 문장 대표값으로 충분
추가 레이어(풀링, 선형 변환 등) 없이 빠르고 단순
거대한 LLM이 학습한 언어 이해력을 그대로 임베딩에 투입


### 4. 학습은 어떻게 했나?

1. 합성 데이터로 예열
	- Qwen3 LLM이 스스로 문장 쌍(질문-답변, 유사.비유사 문장 등 )을 수억 개 생성 -> "비슷한 건 가깝게, 다른 건 멀게"라는 대비/대조 학습(contrastive) 수행.
	- 사람 라벨이 부족해도 저렴하게 방대한 패턴을 학습해 언어.도메인 범용성이 생김
2. 고품질 지도 데이터로 미세 조정
	- 실제 사람이 검수한 정답 쌍(예: 검색 쿼리 <-> 정답 문서)을 투입해 정밀도(precision) 향상
	- 1단계에서 배운 "배운 맞는 감"에 세밀한 기준선을 새겨 넣는 과정
3. 모델 머징(model merging)으로 보정 작업
	- 서로 다른 체크포인트를 **Slerp**(가중치 보간) 방식으로 섞음.
	- 각 버전의 장점(범용성, 정밀성)을 하나의 네트워크에 균형 있게 융합 -> 잡음에 강하고 벤치마크 전반에서 고른 성능


https://arxiv.org/pdf/2506.05176


