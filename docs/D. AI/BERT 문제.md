## 셀프 어텐션의 전체 단계를 설명하라

셀프 어텐션(Self-Attention)은 입력된 문장 내 각 단어가 서로의 중요도를 계산하여 문맥을 반영한 표현을 만드는 과정이다. 

1. **토큰 임베딩과 위치 정보 결합이다** 
	- 먼저 문장의 각 단어를 벡터로 바꾸는 임베딩을 수행한다. 동시에 각 단어의 순서 정보를 담은 위치 인코딩을 더해, 문장 내에서의 상대적 위치를 표현할 수 있도록 준비한다.

2. **쿼리(Query), 키(Key), 밸류(Value)로 분리 투영이다**  
    - 임베딩된 벡터를 세 개의 서로 다른 관점—쿼리, 키, 밸류—로 변환한다. 이 변환을 통해 “누가 묻고”(쿼리), “누가 대답할 수 있는지”(키), “실제 정보를 담고 있는지”(밸류)를 구분한다.

3. **단어 간 유사도(어텐션 스코어) 계산이다**  
    - 각 단어의 쿼리와 다른 모든 단어의 키를 비교하여 유사도를 구한다. 이 유사도는 “이 두 단어가 얼마나 관련이 있는가”를 나타내며, 이후 가중치를 정하는 기준이 된다.

4. **마스킹(선택적 처리)이다**  
    - 번역이나 문장 생성처럼 미래 단어를 알아서는 안 되는 경우, 미래 위치의 유사도는 무시하도록 설정한다. 또 패딩 토큰을 배제하기 위해 마스크를 적용할 수도 있다.

5. **소프트맥스(가중치) 적용이다**  
    =계산된 유사도를 확률 분포로 바꾼다. 이 확률이 바로 “어느 단어에 얼만큼 주목할지”를 나타내는 가중치이다.

6. **밸류와의 가중합으로 새로운 표현 생성이다**  
    밸류 벡터들에 방금 구한 가중치를 곱하고 모두 더한다. 이렇게 하면 각 단어가 문장 전체를 참고하여 다시 표현된 벡터를 얻을 수 있다.

7. **멀티 헤드 어텐션으로 다양한 관점 확보이다**  
    위 과정을 여러 개의 “헤드”로 동시에 수행한다. 각각의 헤드는 다른 관점에서 어텐션을 학습하므로, 합쳐서 처리하면 보다 풍부한 문맥 정보를 담은 표현을 얻는다.

8. **잔차 연결과 정규화로 안정성 부여이다**  
    원래의 임베딩 벡터에 새로 얻은 어텐션 출력을 더하고, 레이어 정규화를 적용한다. 이 단계는 학습을 안정시키고 정보 손실을 방지한다.
