## 인코더 기반이 아닌데도 잘하는 이유
### Decoder 아키텍처 활용 + [EOS] 벡터

Qwen3 기반 디코더 구조 위에, 문장 끝의 `[EOS]` 토큰 마지막 **히든 스테이트**를 바로 임베딩으로 사용.이렇게 하니 LLM의 풍부한 언어 이해 능력을 임베딩으로 그대로 흡수할 수 있습니다.


이게 무슨 말이냐?

### 먼저 토큰,히든 스테이트, 임베딩이 뭔데?

| 용어                        | 설명                            |
| ------------------------- | ----------------------------- |
| **토큰(token)**             | 문장을 잘게 자른 **단어 조각**           |
| **히든 스테이트(hidden state)** | 모델이 각 토큰을 보고 계산한 **머릿속 상태표**  |
| **임베딩(embedding)**        | 문장·단어를 고정 길이 숫자 벡터로 바꾼 **좌표** |
### 디코더는 어떻게 읽고 쓰나?
1. 토큰을 왼쪽 -> 오른쪽 순서로 한 개씩 넣으면서 다음 토큰을 예측
2. 매 스텝마다 "지금까지 본 모든 단어를 요약한 벡터"(히든 스테이트)가 생김

즉, 마지막 스텝(문장 끝 `[EOS]`) 에서 계산된 히든 스테이트엔 
- 앞의 모든 단어 정보
- 문장 전체 맥락
이 압축되어 들어가 있다
